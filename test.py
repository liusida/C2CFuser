# =============================================================
# Cache-to-Cache (C2C) Testing Script
# =============================================================

from utils import (
    load_models_and_tokenizer,
    load_training_dataset,
    split_prompt_response_by_think,
    get_formatted_conversation,
    load_checkpoint,
)


# -------------------------------------------------------------
# Testing Function
# -------------------------------------------------------------
def test(checkpoint_step=0):
    tokenizer, model_rec, model_share, device = load_models_and_tokenizer()
    dataset = load_training_dataset()
    c2c_model_check, _, _ = load_checkpoint(
        step=checkpoint_step, model_rec=model_rec, model_share=model_share, tokenizer=tokenizer
    )
    c2c_model_check.eval()
    p = get_formatted_conversation(dataset["train"][7]["conversations"], tokenizer)
    prompt_text, response_text = split_prompt_response_by_think(p)
    prompt_enc = tokenizer(prompt_text, return_tensors="pt", add_special_tokens=False)
    response_enc = tokenizer(response_text, return_tensors=None, add_special_tokens=False)
    response_len = max(50, len(response_enc["input_ids"]))

    print("=" * 40, "\n== Prompt ==")
    print(prompt_text)

    print("=" * 40, "\n== Expected Response ==")
    print(response_text)

    print("=" * 40, "\n== Actual Generated by C2C ==")
    prompt_input_ids = prompt_enc["input_ids"].to(device)
    prompt_mask = prompt_enc["attention_mask"].to(device)
    g = c2c_model_check.generate(prompt_input_ids, prompt_mask, max_new_tokens=response_len, tokenizer=tokenizer)
    # print(g)
    text = tokenizer.decode(g[0], skip_special_tokens=True)
    print(text)

    print("=" * 40, "\n== Actual Generated by model_rec alone ==")
    g = model_rec.generate(input_ids=prompt_input_ids, attention_mask=prompt_mask, do_sample=False, max_new_tokens=response_len)
    # print(g[0, prompt_input_ids.shape[1]:])
    text = tokenizer.decode(g[0, prompt_input_ids.shape[1] :], skip_special_tokens=True)
    print(text)

    print("=" * 40, "\n== Actual Generated by model_share alone ==")
    g = model_share.generate(input_ids=prompt_input_ids, attention_mask=prompt_mask, do_sample=False, max_new_tokens=response_len)
    # print(g[0, prompt_input_ids.shape[1]:])
    text = tokenizer.decode(g[0, prompt_input_ids.shape[1] :], skip_special_tokens=True)
    print(text)

    print("=" * 40, "\nRank them. Ignore cut-off and <!-- debug messages -->.")


if __name__ == "__main__":
    test(checkpoint_step=8)
